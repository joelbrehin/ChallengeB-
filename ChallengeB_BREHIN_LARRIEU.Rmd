---
title: "Challenge B"
author: "Vincent Larrieu - Joel Brehin"
date: "December 8, 2017"
output: pdf_document
fontsize: 11pt
---

Link to gihub repository :      https://github.com/VincentLarr/ChallengeB-

DISCLAIMER: Some of the data files are too big to be uploaded on github, this work was realized using a R project file created from a repository containing this document as well as all the datasets used. Through the course of running the code three prompts will ask for data input.   
In the first one, one must choose "train" file from challenge A, the second is "test" file from challenge A and the third is the CIL Database named "OpenCNIL_Organismes_avec_CIL_VD_20171115". Finally in last exercise, step 3, the line number 406 must contain the name of the SIREN database on the user PC (in our case we kept the default  "sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv"). We assume that to work this file needs to be in the same repository than this RMD file.

#TASK 1B
##Step 1 
Random forest is a learning method for regression, classification and other tasks. The goal is to create a model that predicts the value of a target variable based on several independent variables. It constructs a multitude of decision tree at training time and outputting the class that is the mean prediction of the individual tree. In case of a regression, each interior node represents one of the independent variables such as "lotsize" for instance. Each leaf represents a value of the target variable, which "price" in our case, given the value of all the independent variables. Finally, the different paths represents different combination of values of the independent variables.

##Step2
Now we train the machine learning "randomForest" on the data train. 

```{r, echo=FALSE, include=FALSE}
load.libraries <- c('tidyverse','randomForest','e1071','caret','np', 'data.table','dplyr')
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependencies = TRUE)
sapply(load.libraries, require, character = TRUE)



train <- read.csv(file=file.choose(), header=T, sep=",", dec=".", stringsAsFactors=FALSE)
attach(train)


train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
gather(key = "feature",value = "missing.observations") %>% 
filter(missing.observations > 0)


#removing variables that contains more than 100 missing values 
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.))))%>% 
gather (key = "feature", value = "missing.observations") %>% 
filter(missing.observations > 100) %>% select(feature) %>% unlist

train <- train %>% select(- one_of(remove.vars))


#removing rows that contains missing values. 
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
gather(key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 100) %>% select(feature) %>% unlist

train <- train %>% select(- one_of(remove.vars))



train <- train %>% filter(is.na(GarageType) == FALSE,
is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, 
is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)
# remove rows with NA in some of these variables, 
#check if you take all missing values like this


# Check for duplicated rows.
#cat("The number of duplicated rows are", nrow(train) - #"nrow(unique(train)))

#Convert character to factors 
cat_var <- train %>% summarise_all(.funs = funs(is.character(.))) %>% 
gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% 
select(feature) %>% unlist
# cat_var is the vector of variable names that are stored as character

train %>% mutate_at(.cols = cat_var, .funs = as.factor)
# i transform them all to factor.
```


We have kept the model that we used in the Challenge A and we run the RandomForest machine learning on the data train.  

```{r, echo=FALSE, include=FALSE}
#we keep the model used in the challenge A to make the prediction. 
model4 <-lm(SalePrice~ LotArea + OverallQual + OverallCond + YearBuilt
          +X1stFlrSF+ X2ndFlrSF +GarageArea+ ScreenPorch , data =train)

```
```{r, echo=TRUE}

RFtrain <- randomForest(SalePrice~ LotArea + OverallQual + OverallCond + YearBuilt
                        +X1stFlrSF+ X2ndFlrSF +GarageArea+ ScreenPorch , data=train)
```


##step3
```{r, echo=FALSE, include=FALSE}
# we upload the data set to make predictions. 
test <- read.csv(file=file.choose(), header=T, sep=",", dec=".", stringsAsFactors=FALSE)

#attach(test)

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
  gather(key = "feature",value = "missing.observations") %>% 
  filter(missing.observations > 0)


#removing variables that contains more than 100 missing values 
remove.vars.test <- test %>% summarise_all(.funs = funs(sum(is.na(.))))%>% 
  gather (key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 100) %>% select(feature) %>% unlist

test <- test %>% select(- one_of(remove.vars.test))


#removing rows that contains missing values. 
remove.vars.test <- test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
  gather(key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 100) %>% select(feature) %>% unlist

test <- test %>% select(- one_of(remove.vars.test))

#show which variables have missing values 
test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
  gather(key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 0)

test <- test %>% filter(is.na(GarageType) == FALSE,
                        is.na(MasVnrType) == FALSE, 
                        is.na(BsmtFinType2) == FALSE, 
                        is.na(BsmtExposure) == FALSE, 
                        is.na(Electrical) == FALSE,
                        is.na(MSZoning) == FALSE,
                        is.na(Utilities) == FALSE,
                        is.na(Exterior1st) == FALSE,
                        is.na(Exterior2nd) == FALSE,
                        is.na(MasVnrArea) == FALSE,
                        is.na(BsmtQual) == FALSE,
                        is.na(BsmtCond) == FALSE,
                        is.na(BsmtExposure) == FALSE,
                        is.na(BsmtFinSF1) == FALSE,
                        is.na(BsmtFinType1) == FALSE,
                        is.na(BsmtFinSF1) == FALSE,
                        is.na(BsmtFinSF2) == FALSE,
                        is.na(TotalBsmtSF) == FALSE,
                        is.na(TotalBsmtSF) == FALSE,
                        is.na(BsmtFullBath) == FALSE,
                        is.na(KitchenQual) == FALSE,
                        is.na(Functional) == FALSE,
                        is.na(GarageType) == FALSE,
                        is.na(GarageYrBlt) == FALSE,
                        is.na(GarageFinish) == FALSE,
                        is.na(GarageCars) == FALSE,
                        is.na(GarageArea) == FALSE,
                        is.na(GarageQual) == FALSE,
                        is.na(GarageCond) == FALSE,
                        is.na( SaleType ) == FALSE)

# remove rows with NA in some of these variables, 
#check if you take all missing values like this

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
  gather(key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 0)

# Check for duplicated rows.
cat("The number of duplicated rows are", nrow(test) - nrow(unique(test)))

#Convert character to factors 
cat_var_test <- test %>% summarise_all(.funs = funs(is.character(.))) %>% 
  gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% 
  select(feature) %>% unlist
# cat_var is the vector of variable names that are stored as character

test %>% mutate_at(.cols = cat_var, .funs = as.factor)
# i transform them all to factors
```

```{r, echo=TRUE}
prediction.lr <- predict(model4, test)

```
I predict the Saleprice with the randomForest function:
```{r, echo=TRUE, message=FALSE}
predict.rf <- predict(RFtrain, test)

``` 
We can compare first with the two summaries :
```{r, echo=FALSE, message=FALSE}

summary(predict.rf)

summary(prediction.lr)
``` 
The prediction with the regression is more generous. Values are on average higher than the prediction with random forest. Moreover, the prediction with random forest doesn't give negative values compare to the prediction with the linear prediction, which is an enormous advantage because we try to predict a price.  

We can also compare with box plot:

```{r, echo=FALSE, message =FALSE}
boxplot(prediction.lr, outline = FALSE, horizontal = TRUE)
boxplot(predict.rf, outline = FALSE, horizontal = TRUE)
```

#TASK 2B
```{r, include=FALSE, echo=FALSE}

## Set-up (From challenge A)
library(tidyverse)
library(caret)
library(np)
library(data.table)
library(dplyr)
set.seed(1)
x<-rnorm(150,0,1)
error<-rnorm(150,0,1)
xcube<-x^3 #no need for lapply as R works element by element
y<-xcube+error
rddraw<-cbind.data.frame(y,x,xcube,error)
training_index <- createDataPartition(rddraw$y, p=0.80, list=FALSE)
testing <- slice(rddraw, -training_index)
training <- slice(rddraw, training_index)
  rddraw$subsample<-ifelse(rddraw$x %in% training$x & rddraw$y%in% training$y & rddraw$error %in% training$error,"TRAINING", "TESTING")

```

##Step1
 We simply use the function npreg from the package np with the following arguments:

```{r, echo=TRUE, include=TRUE, message=FALSE}
set.seed(1)
ll.fit.lowflex<-npreg(y~x,bws = 0.5,regtype="ll",data=training,newdata=training)

```

##Step2
Same than previously, changing the bandwith
```{r, echo=TRUE, message=FALSE, include=TRUE}

ll.fit.highflex<-npreg(y~x,bws = 0.01,regtype="ll",data=training,newdata=training)

```

##Step 3
We add columns to our training data frame containing the predictions using both regressions from the previous questions.
Then we simply plot them as well as the points and the theoretical value x^3. In blue is the high flexibility line, in red the low flexibility one

```{r, echo=FALSE, message=FALSE, include=TRUE}
training$ll.fit.lowflex<-predict(object = ll.fit.lowflex, newdata=training)
training$ll.fit.highflex<-predict(object = ll.fit.highflex, newdata=training)

ggplot(data=training)+
  geom_point(mapping=aes(x=training$x,y=training$y))+
  geom_line(mapping = aes(x=training$x,y=training$ll.fit.lowflex),color="red")+
  geom_line(mapping = aes(x=training$x,y=training$ll.fit.highflex),color="blue")+
  geom_line(mapping = aes(x=training$x,y=training$xcube))

```

##Step 4

The predictions from the high flexibility model have less bias, indeed the blue line is closer from the points for every x.However this blue line is also the more variable as it doesn't follow a trend close to the cube function, as is the case with the red line

##Step 5
We proceed like in question 3 to build the plot, using the predictions of the old model on the new data(not making a new model with the new data)


```{r, echo=TRUE, message=FALSE, include=TRUE}

testing$ll.fit.lowflex<-predict(object = ll.fit.lowflex, newdata=testing)
testing$ll.fit.highflex<-predict(object = ll.fit.highflex, newdata=testing)
```

```{r, echo=FALSE, message=FALSE, include=TRUE}
ggplot(data=testing)+
  geom_point(mapping=aes(x=testing$x,y=testing$y))+
  geom_line(mapping = aes(x=testing$x,y=testing$ll.fit.lowflex),color="red")+
  geom_line(mapping = aes(x=testing$x,y=testing$ll.fit.highflex),color="blue")+
  geom_line(mapping = aes(x=testing$x,y=testing$xcube))


```

We also see that the high-flexibility model is no longer the least-biased. For most points, the red line is closer from the dots than the blue one.
Using a highly-flexible model on new data doesn't seem to be a good way to get a good prediction.

##Step 6
We do this using a simple sequence command

```{r, echo=TRUE, message=FALSE}
bwsvec<-seq(0.01,0.5,0.001)
```

##Step7
For each element in our bandwith vector we want to apply a npregfunction so we can do this using a lapply.

```{r, echo=TRUE, message=FALSE}

npreglist<-lapply(X=bwsvec, FUN=function(bwsvec){npreg(y~x, method="ll",
                                                       bws=bwsvec, data=training)})
```

##Step8
We first create a function which from a model input gives the MSE of this model on new data(here training),we name this function get MSE, from this we can just do a sapply using our list of regression as the object and the new function getMSE
```{r, include=FALSE,echo=TRUE, message=FALSE}
n1<-dim(training)[1]

GetMSEtraining<-function(model){
 (sum((predict(object = model,newdata=training)-training$y)^2))/n1
}
MSEtraining<-sapply(X=npreglist,FUN=GetMSEtraining)
class(MSEtraining)
```

##Step9
We proceed similarly, except this time our "get MSE" function inputs the testing data as the new data.
```{r,include=FALSE, echo=TRUE, message=FALSE}
n2<-dim(testing)[1]

GetMSEtesting<-function(model){
  (sum((predict(object = model,newdata=testing)-testing$y)^2))/n2
}
MSEtesting<-sapply(X=npreglist,FUN=GetMSEtesting)
class(MSEtesting)
```

##Step10
We plot the MSE according to the bandwith. The orange line is the MSE in the testing data(the new data) and the blue one is the one on the training data (the one on which the model was fitted). MSE is a measure of bias, the lower the MSE, the lower the bias.  
As we've seen before a higher flexibility(a higher bandwith) will reduce the bias with the training data, however this effect will stop after a certain bandwith and be replaced by an increasing bias. For the testing data, theeffect is the opposite, the higher the bandwith the morebiased the model, this is explained by the fact that fitting closely a model with some data doesn't mean that this relationship will hold with another data.
```{r, echo=FALSE, message=FALSE, include=TRUE}
ggplot()+
  geom_line(mapping = aes(x=bwsvec,y=MSEtraining),color="blue")+
  geom_line(mapping = aes(x=bwsvec,y=MSEtesting),color="orange")
```

#TASK 3B


##Step1
We import the CIL dataset as usual being careful that the separator is the good one (here a semi column).
```{r,include=FALSE, echo=TRUE, message=FALSE}
CIL<- read.csv(file=file.choose(), header=T, dec=".",sep=";")

```


##Step2
First we reduce the Postcodes to the first three digits( we need three to separate between mainland France and overseas territories).Indeed, overseas territories have a three digit code, for simplicity reasons, we focus on mainland France. We create a data partition for metropolitan France, excluding  overseas territories with 3digits  department numbers and French companies abroad (above post code 959).

```{r, include=FALSE, echo=TRUE, message=TRUE}
CIL$Code_Postal<-(substr(CIL$Code_Postal,0,3))
```


```{r, include=FALSE,echo=TRUE, message=FALSE}
OMetEtranger<-CIL[CIL$Code_Postal>"959",]
Metropole<-CIL[CIL$Code_Postal<"959",]
```
We then reduce mainland postcodes to the  two first digits
```{r,include= FALSE, echo=TRUE, message=FALSE }
Metropole$Departement<-(substr(Metropole$Code_Postal,0,2))
#We create a vector giving the department numbers as chacracters(as it is recorded in our dataset).
```

```{r, include=FALSE, echo=TRUE, message=FALSE }
DepartementmetroA<-c("01","02","03","04","05","06","07","08","09")
DepartementmetroB<-(as.character(c("10":"95")))
Departementmetro<-c(DepartementmetroA,DepartementmetroB)
Departementmetro

```
We then simply finish by having a loop coutning for each element of the department vectors, how many lines have this value in their departement column, and we get the following table:
```{r,include=FALSE, echo=TRUE, message=FALSE }


Number.of.CIL<-rep(0,95)

for(i in Departementmetro){
Number.of.CIL[as.numeric(i)]<-nrow(Metropole[Metropole$Departement==Departementmetro[as.numeric(i)],])
}
```
```{r, include=TRUE, echo=FALSE, message=FALSE }

Table<-rbind.data.frame(Departement.code=Departementmetro,Number.of.CIL)
colnames(Table)<-NULL
rownames(Table)<-c("Departement","Number of CIL")
Table
```

##Step3
The dataframe is too large to store it in the RAM so we need to read it by chunck of 100000 observations.To do that we use a repeat function that:  
* removes the oldest lines for duplicates SIREN numbers(first order command by date,then only move to a new dataset those who are not duplicates)  
* Select the rows whose SIREN number is also is the CIL database  
*Pastes this trimmed sample to an object call DataFinal that will contain the results of all sets.    
The size of the chunck can be changed if it is a too large number to be stored on the RAM.  
```{r, include=FALSE, echo=TRUE, message=FALSE }
start<-Sys.time()
SIRENdatabase<-'sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv'
chunksize<-100000
connector<-file(description = SIRENdatabase, open="r")
index<-0

initiator<-read.table(connector,nrows=2,skip=0,header=TRUE,fill=T,sep=";")
ColNAMES<-names(initiator)
DataFINAL<- data.frame(matrix(ncol = length(ColNAMES), nrow = 0))
colnames(DataFINAL) <- ColNAMES

BizCIL<-c(CIL$ï..Siren)

repeat{index<-index+1
print(paste('Processing rows:',index*chunksize))

SIRENchunckRAW<-read.table(connector,nrows = chunksize,skip=0,header = F,fill=T,sep=";",col.names=ColNAMES,stringsAsFactors = FALSE)

SIRENchunckRAW<-SIRENchunckRAW[order(SIRENchunckRAW[,'DATEMAJ'],decreasing = TRUE),]
SIRENchunckreduced1<-SIRENchunckRAW[which(!duplicated(SIRENchunckRAW$SIREN)),]
SIRENchunckreduced2<-SIRENchunckreduced1[SIRENchunckreduced1$SIREN %in%BizCIL ,]
DataFINAL<-rbind(DataFINAL,SIRENchunckreduced2)
if (nrow(SIRENchunckRAW)!= chunksize){
  print('Finished')
  break}
}

close(connector)

DataFINAL
```
This takes this amount of time:
```{r,echo=FALSE, include=TRUE}
end<-Sys.time()
end-start
```
We just have to bind the two datasets using the left_join command rom dplyr packages
```{r,include=FALSE, echo=FALSE, message=FALSE }
library(dplyr)
CIL$SIREN<-CIL$ï..Siren
DataMERGED<-left_join(CIL,DataFINAL,by="SIREN")
```
##Step4
We plot the histogram with ggplot 2, however we cannot order the bins as they are a character variable than cannot be ordered.  
```{r, include=TRUE,echo=FALSE, message=FALSE, include=TRUE }

p<-ggplot(DataMERGED)+
  geom_bar(aes(LIBTEFEN))
p+theme(axis.text.x= element_text(angle = 60))
```


