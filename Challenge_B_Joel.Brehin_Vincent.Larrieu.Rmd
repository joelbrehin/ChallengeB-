---
title: "ChallengeB"
author: "Vincent Larrieu - Joël Bréhin"
date: "6 décembre 2017"
output: html_document
---

####link to gihub repository :      git@github.com:VincentLarr/ChallengeB-.git

##Step 1 
#####Random forest is a learning method for regression, classification and other tasks. The goal is to create a model that predicts the value of a target variable based on several independent variables. It constructs a multitude of decision tree at training time and outputting the class that is the mean prediction of the individual tree. In case of a regression, each interior node represents one of the independent variables such as "lotsize" for instance. Each leaf represents a value of the target variable, which "price" in our case, given the value of all the independent variables. Finally, the different paths represents different combination of values of the independent variables.

##step2
#####Now we train the machine learning "randomForest" on the data train. 
```{r echo=FALSE, include=TRUE}
load.libraries <- c('tidyverse', 'randomForest','e1071', 'caret','np', 'data.table','dplyr' )
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependencies = TRUE)
sapply(load.libraries, require, character = TRUE)



train <- read.csv(file=file.choose(), header=T, sep=",", dec=".", stringsAsFactors=FALSE)
attach(train)


train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
gather(key = "feature",value = "missing.observations") %>% 
filter(missing.observations > 0)


#removing variables that contains more than 100 missing values 
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.))))%>% 
gather (key = "feature", value = "missing.observations") %>% 
filter(missing.observations > 100) %>% select(feature) %>% unlist

train <- train %>% select(- one_of(remove.vars))


#removing rows that contains missing values. 
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
gather(key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 100) %>% select(feature) %>% unlist

train <- train %>% select(- one_of(remove.vars))



train <- train %>% filter(is.na(GarageType) == FALSE,
is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, 
is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)
# remove rows with NA in some of these variables, 
#check if you take all missing values like this


# Check for duplicated rows.
#cat("The number of duplicated rows are", nrow(train) - #"nrow(unique(train)))

#Convert character to factors 
cat_var <- train %>% summarise_all(.funs = funs(is.character(.))) %>% 
gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% 
select(feature) %>% unlist
# cat_var is the vector of variable names that are stored as character

train %>% mutate_at(.cols = cat_var, .funs = as.factor)
# i transform them all to factor.
```


#####we have kept the model that we used in the Challenge A. 
#####Here the model that we used :
#####SalePrice~ LotArea + OverallQual + OverallCond + YearBuilt           +X1stFlrSF+ X2ndFlrSF +GarageArea+ ScreenPorch  

```{r, echo=FALSE, include=TRUE}
#we keep the model used in the challenge A to make the prediction. 
model4 <-lm(SalePrice~ LotArea + OverallQual + OverallCond + YearBuilt
          +X1stFlrSF+ X2ndFlrSF +GarageArea+ ScreenPorch , data =train)

```

#####Now, we run the RandomForest machine learning on the data train. 
```{r, echo=TRUE}

RFtrain <- randomForest(SalePrice~ LotArea + OverallQual + OverallCond + YearBuilt+X1stFlrSF+ X2ndFlrSF +GarageArea+ ScreenPorch , data=train)


```


##step3
#####now we predict the sale price on the dataset test
```{r, echo=FALSE, include=TRUE}
# we upload the data set to make predictions. 
test <- read.csv(file=file.choose(), header=T, sep=",", dec=".", stringsAsFactors=FALSE)

#attach(test)

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
  gather(key = "feature",value = "missing.observations") %>% 
  filter(missing.observations > 0)


#removing variables that contains more than 100 missing values 
remove.vars.test <- test %>% summarise_all(.funs = funs(sum(is.na(.))))%>% 
  gather (key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 100) %>% select(feature) %>% unlist

test <- test %>% select(- one_of(remove.vars.test))


#removing rows that contains missing values. 
remove.vars.test <- test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
  gather(key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 100) %>% select(feature) %>% unlist

test <- test %>% select(- one_of(remove.vars.test))

#show which variables have missing values 
test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
  gather(key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 0)

test <- test %>% filter(is.na(GarageType) == FALSE,
                        is.na(MasVnrType) == FALSE, 
                        is.na(BsmtFinType2) == FALSE, 
                        is.na(BsmtExposure) == FALSE, 
                        is.na(Electrical) == FALSE,
                        is.na(MSZoning) == FALSE,
                        is.na(Utilities) == FALSE,
                        is.na(Exterior1st) == FALSE,
                        is.na(Exterior2nd) == FALSE,
                        is.na(MasVnrArea) == FALSE,
                        is.na(BsmtQual) == FALSE,
                        is.na(BsmtCond) == FALSE,
                        is.na(BsmtExposure) == FALSE,
                        is.na(BsmtFinSF1) == FALSE,
                        is.na(BsmtFinType1) == FALSE,
                        is.na(BsmtFinSF1) == FALSE,
                        is.na(BsmtFinSF2) == FALSE,
                        is.na(TotalBsmtSF) == FALSE,
                        is.na(TotalBsmtSF) == FALSE,
                        is.na(BsmtFullBath) == FALSE,
                        is.na(KitchenQual) == FALSE,
                        is.na(Functional) == FALSE,
                        is.na(GarageType) == FALSE,
                        is.na(GarageYrBlt) == FALSE,
                        is.na(GarageFinish) == FALSE,
                        is.na(GarageCars) == FALSE,
                        is.na(GarageArea) == FALSE,
                        is.na(GarageQual) == FALSE,
                        is.na(GarageCond) == FALSE,
                        is.na( SaleType ) == FALSE)
#to be completed
# remove rows with NA in some of these variables, 
#check if you take all missing values like this

test %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% 
  gather(key = "feature", value = "missing.observations") %>% 
  filter(missing.observations > 0)

# Check for duplicated rows.
cat("The number of duplicated rows are", nrow(test) - nrow(unique(test)))

#Convert character to factors 
cat_var_test <- test %>% summarise_all(.funs = funs(is.character(.))) %>% 
  gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% 
  select(feature) %>% unlist
# cat_var is the vector of variable names that are stored as character

test %>% mutate_at(.cols = cat_var, .funs = as.factor)
# i transform them all to factors
```

##### ... in order to predict the Sale price with the dataset test
```{r, echo=TRUE}
prediction.lr <- predict(model4, test)

```
##### I predict the Saleprice with the randomForest function
```{r, echo=TRUE, message=FALSE}
predict.rf <- predict(RFtrain, test)

``` 
##### we can compare first with the two summaries 
```{r, echo=TRUE, message=FALSE}

summary(predict.rf)

summary(prediction.lr)
``` 
##### the prediction with the regression is more generous. Values are on average higher than the prediction with random forest. Moreover, the prediction with random forest doesn't give negative values compare to the prediction with the linear prediction, which is an enormous advantage because we try to predict a price.  

#####we can also compare with box plot

```{r, echo=TRUE, message =FALSE}
boxplot(prediction.lr, outline = FALSE, horizontal = TRUE)
boxplot(predict.rf, outline = FALSE, horizontal = TRUE)
```

##TASK 2B
```{r setup, include=TRUE, echo=FALSE}

## Set-up (From challenge A)
set.seed(1)
x<-rnorm(150,0,1)
x
error<-rnorm(150,0,1)
error
xcube<-x^3 #no need for lapply as R works element by element
xcube
y<-xcube+error
y
rddraw<-cbind.data.frame(y,x,xcube,error)
training_index <- createDataPartition(rddraw$y, p=0.80, list=FALSE)
testing <- slice(rddraw, -training_index)
training <- slice(rddraw, training_index)
training
  rddraw
  rddraw$subsample<-ifelse(rddraw$x %in% training$x & rddraw$y%in% training$y & rddraw$error %in% training$error,"TRAINING", "TESTING")

```

##Step1
##### We simply use the function npreg from the package np with the following arguments:

```{r, echo=TRUE, include=TRUE, message=FALSE}
set.seed(1)
ll.fit.lowflex<-npreg(y~x,bws = 0.5,regtype="ll",data=training,newdata=training)
summary(ll.fit.lowflex)
```

##Step2
##### We simply use the function npreg from the package np with the following arguments:

```{r, echo=TRUE, message=FALSE, include=TRUE}

ll.fit.highflex<-npreg(y~x,bws = 0.01,regtype="ll",data=training,newdata=training)
summary(ll.fit.highflex)
```

##Step 3
#####We add columns to our training data frame containing the predictions using both regressions from the previous questions.
#####Then we simply plot them as well as the points and the theoretical value x^3.

```{r, echo=TRUE, message=FALSE, include=TRUE}
training$ll.fit.lowflex<-predict(object = ll.fit.lowflex, newdata=training)
training$ll.fit.highflex<-predict(object = ll.fit.highflex, newdata=training)

ggplot(data=training)+
  geom_point(mapping=aes(x=training$x,y=training$y))+
  geom_line(mapping = aes(x=training$x,y=training$ll.fit.lowflex),color="red")+
  geom_line(mapping = aes(x=training$x,y=training$ll.fit.highflex),color="blue")+
  geom_line(mapping = aes(x=training$x,y=training$xcube))

```
##Step4 
#####The predictions from the high flexibility model have less bias, indeed the blue line is closer from the points for every x.
#####However this blue line is also the more variable as it doesn't follow a trend close to the cube function, as is the case wuth the red line

##Step 5
#####We proceed like in question 3 to build the plot, using the predictions of the old model on the new data(not making a new model with the new data)


```{r, echo=TRUE, message=FALSE, include=TRUE}

testing$ll.fit.lowflex<-predict(object = ll.fit.lowflex, newdata=testing)
testing$ll.fit.highflex<-predict(object = ll.fit.highflex, newdata=testing)

ggplot(data=testing)+
  geom_point(mapping=aes(x=testing$x,y=testing$y))+
  geom_line(mapping = aes(x=testing$x,y=testing$ll.fit.lowflex),color="red")+
  geom_line(mapping = aes(x=testing$x,y=testing$ll.fit.highflex),color="blue")+
  geom_line(mapping = aes(x=testing$x,y=testing$xcube))


```

#####We also see that the high-flexibility model is no longer the least-biased. For most points, the red line is closer from the dots than the blue one.
#####Using a highly-flexible model on new data doesn't seem to be a good way to get a good prediction.

##Step 6
#####We do this using a simple sequence command

```{r, echo=TRUE, message=FALSE}
bwsvec<-seq(0.01,0.5,0.001)
```

##Step7
#####For each element in our bandwith vector we want to apply a npregfunction so we can do this using a lapply.

```{r, echo=TRUE, message=FALSE}

npreglist<-lapply(X=bwsvec, FUN=function(bwsvec){npreg(y~x, method="ll", bws=bwsvec, data=training)})
```

##Step8
#####We firs creat a function which from a model input gives the MSE of this model on new data(here training),
#####we name this function get MSE, from this we can just do a sapply using our list of regression as the object and the new function
#####getMSE
```{r, echo=TRUE, message=FALSE}
n1<-dim(training)[1]

GetMSEtraining<-function(model){
 (sum((predict(object = model,newdata=training)-training$y)^2))/n1
}
MSEtraining<-sapply(X=npreglist,FUN=GetMSEtraining)
class(MSEtraining)
```

##Step9
#####We proceed similarly, except this time our get MSe function inputs the testing data as the new data.
```{r, echo=TRUE, message=FALSE}
n2<-dim(testing)[1]

GetMSEtesting<-function(model){
  (sum((predict(object = model,newdata=testing)-testing$y)^2))/n2
}
MSEtesting<-sapply(X=npreglist,FUN=GetMSEtesting)
class(MSEtesting)
```

##Step10
#####We plot the MSE according to the bandwith. The orange line is the #####MSE is the testing data(the new data) and the blue one is 
#####the one on the training data (the one on which the model was #####fitted). MSE is a measure of bias, the lower the MSE, the lower #####the bias.
#####As we've seen before a higher flexibility(a higher bandwith) will #####reduce the bias with the training data, however this effect will #####stop after a certain bandwith 
#####and be replaced by an increasing bias. For the testing data, the #####effect is the opposite, the higher the bandwith the more #####biased the model, 
#####this is explained by the fact that fitting closely a model with #####some data doesn't mean that this relationship will hold with more #####data.
```{r, echo=TRUE, message=FALSE, include=TRUE}
ggplot()+
  geom_line(mapping = aes(x=bwsvec,y=MSEtraining),color="blue")+
  geom_line(mapping = aes(x=bwsvec,y=MSEtesting),color="orange")
```

##TASK 3B
#This exercice has been made within a R.project call "Challenge" created after a file including the datasets the two datasets

##Step1
#####We import the dataset as usual being careful that the separator is the good one (here a semi column)
```{r, echo=TRUE, message=FALSE}
CIL<- read.csv(file=file.choose(), header=T, dec=".",sep=";")

```


##Step2
#####First we reduce the Postcodes to the first three digits( we need three to separate between mainland France and overseas territories)
```{r, echo=TRUE, message=TRUE}
CIL$Code_Postal<-(substr(CIL$Code_Postal,0,3))
```
#####Indeed, overseas territories have a three digit code, for simplicity reasons, we focus on mainland France.
#####We create a data partition for metropolitan France, excluding  overseas territories with 3digits department numbers and French companies abroad.

```{r, echo=TRUE, message=FALSE}
OMetEtranger<-CIL[CIL$Code_Postal>"959",]
Metropole<-CIL[CIL$Code_Postal<"959",]
```
#We then reduce mainland postcodes to the  two first digits
```{r, echo=TRUE, message=FALSE }
Metropole$Departement<-(substr(Metropole$Code_Postal,0,2))
#We create a vector giving the department numbers as chacracters(as it is recorded in our dataset).
```

```{r, echo=TRUE, message=FALSE }
DepartementmetroA<-c("01","02","03","04","05","06","07","08","09")
DepartementmetroB<-(as.character(c("10":"95")))
Departementmetro<-c(DepartementmetroA,DepartementmetroB)
Departementmetro

```

```{r, echo=TRUE, message=FALSE }
#We then simply finish by having a loop coutning for each element of the department vectors, how many lines have this value
#in thier departement column.
```

```{r, echo=TRUE, message=FALSE }
Number.of.CIL<-rep(0,95)

for(i in Departementmetro){
Number.of.CIL[as.numeric(i)]<-nrow(Metropole[Metropole$Departement==Departementmetro[as.numeric(i)],])
}
```
#We then simply make a table (Rossi dit nice table, je sais pas si comme Ã§a Ã§a suffit, je sais pas comment changer l'esthÃ©tique)
```{r, echo=TRUE, message=FALSE }
Table<-cbind.data.frame(Deparement.code=Departementmetro,Number.of.CIL)
Table
```
#####Step3
#The dataframe is too large to store it in the RAM so we nedd to read it by chunck of 100000 observations.
#To do that we use a repeat function that:
#-removes the oldest lines for duplicates SIREN numbers(first order command by date,
# then only move to a new dataset those who are not duplicates)
#- Select the rows whose SIREN number is also is the CIL database
#Pastes this trimmed sample to an object call DataFinal that will contain the results of all sets. 

#####The whole procees takes roughly eleven minutes, the size of the chunck can be changed if it is a too large number to be stored on the RAM
```{r, echo=TRUE, message=FALSE }
start<-Sys.time()
SIRENdatabase<-'sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv'
chunksize<-100000
connector<-file(description = SIRENdatabase, open="r")
index<-0

initiator<-read.table(connector,nrows=2,skip=0,header=TRUE,fill=T,sep=";")
ColNAMES<-names(initiator)
DataFINAL<- data.frame(matrix(ncol = length(ColNAMES), nrow = 0))
colnames(DataFINAL) <- ColNAMES

BizCIL<-c(CIL$Ã¯..Siren)

repeat{index<-index+1
print(paste('Processing rows:',index*chunksize))

SIRENchunckRAW<-read.table(connector,nrows = chunksize,skip=0,header = F,fill=T,sep=";",col.names=ColNAMES,stringsAsFactors = FALSE)

SIRENchunckRAW<-SIRENchunckRAW[order(SIRENchunckRAW[,'DATEMAJ'],decreasing = TRUE),]
SIRENchunckreduced1<-SIRENchunckRAW[which(!duplicated(SIRENchunckRAW$SIREN)),]
SIRENchunckreduced2<-SIRENchunckreduced1[SIRENchunckreduced1$SIREN %in%BizCIL ,]
DataFINAL<-rbind(DataFINAL,SIRENchunckreduced2)
if (nrow(SIRENchunckRAW)!= chunksize){
  print('Finished')
  break}
}

close(connector)

DataFINAL

end<-Sys.time()
end-start
```
#####We just have to bind the two datasets using the left_join command rom dplyr packages
```{r, echo=TRUE, message=FALSE }
CIL$SIREN<-CIL$Ã¯..Siren
DataMERGED<-left_join(CIL,DataFINAL,by="SIREN")
```
##Step4
```{r, echo=TRUE, message=FALSE, include=TRUE }
ggplot(DataMERGED)+
  geom_bar(aes(LIBTEFEN))
```



